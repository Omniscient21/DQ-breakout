{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bcdc4a0",
   "metadata": {},
   "source": [
    "#### Breakout\n",
    "Code guided from:\n",
    "* https://www.nature.com/articles/nature14236\n",
    "* https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
    "* https://github.com/fg91/Deep-Q-Learning/blob/master/DQN.ipynb\n",
    "* https://github.com/openai/baselines/tree/master/baselines/deepq\n",
    "* https://github.com/jasonbian97/Deep-Q-Learning-Atari-Pytorch\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "949955cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image \n",
    "from tqdm.notebook import tqdm\n",
    "from collections import deque\n",
    "import itertools\n",
    "import random\n",
    "import baselines_wrappers # https://github.com/openai/baselines/tree/master/baselines/deepq\n",
    "import gym\n",
    "from collections import namedtuple\n",
    "from pytorch_wrappers import make_atari_deepmind,BatchedPytorchFrameStack,PytorchLazyFrames\n",
    "from torch.utils.tensorboard import SummaryWriter \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e07e0b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "0\n",
      "NVIDIA GeForce RTX 3080\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfb9dcc",
   "metadata": {},
   "source": [
    "### Hyperparamters\n",
    "\n",
    "Based on \n",
    "\n",
    "https://www.nature.com/articles/nature14236 *Human-level control through deep reinforcement\n",
    "learning*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8abc0d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.99                  # discount rate \n",
    "batch_size = 32               # batch size\n",
    "memory_size = 1000000         # mem capacity of our replay memory\n",
    "required_mem = 50000          # init-size how much we want in the memory before training \n",
    "e_start = 1.0                 # start value of epsilon \n",
    "e_end =  0.1                  # end value of epsilon \n",
    "e_steps = 1000000             # number of steps for epsilon reaches end\n",
    "target_update = 2500          # how often we update target network to action network\n",
    "lr = 25e-5                    # learning rate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff6c7f8",
   "metadata": {},
   "source": [
    "# Preprocess\n",
    "84x84x1 as used in paper \n",
    "https://arxiv.org/pdf/1312.5602.pdf \n",
    "\n",
    "4.1 Preprocessing and Model Architecture\n",
    "\n",
    "The raw frames are preprocessed by first converting their RGB representation\n",
    "to gray-scale and down-sampling it to a 110×84 image.\n",
    "\n",
    "The final input representation is obtained by\n",
    "cropping an 84 × 84 region of the image that roughly captures the playing area\n",
    "\n",
    " note: tensor has channel as first dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5edc2302",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Compose, ToTensor, Normalize, Grayscale, Resize\n",
    "# No samples for calculate mean std for normalisaiton - skip\n",
    "class Preprocess():\n",
    "    def __init__(self, target_width = 84, target_height = 84):\n",
    "        self.width = target_width\n",
    "        self.height = target_height\n",
    "        \n",
    "        # transform\n",
    "        self.transform = T.Compose([\n",
    "            ToTensor(),\n",
    "            Grayscale(),\n",
    "            Resize((110,84))\n",
    "        ])\n",
    "    \n",
    "    def process(self,image):\n",
    "        #return self.transform(image)\n",
    "        # crop to play area not clear from paper exactly which area is considered\n",
    "        # tested different values\n",
    "        return T.functional.crop(self.transform(image),18,0,self.height,self.width)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c583f9",
   "metadata": {},
   "source": [
    "# Replay Memory\n",
    "We don't want to just learn from the previous states actiosn since they are highly correlated and the network would \"forget\" old states actions \n",
    "So we sample a batch from this memory\n",
    "\n",
    "## notes from nature paper\n",
    "**memory of 1 million most recent frames**\n",
    "\n",
    "First, we use a technique known as experience replay in which we store the\n",
    "agent’s experiences at each time-step, $e_t = (s_t, a_t,r_t,s_{t+1})$, in a data set $D_t = \\{e_1,…,e_t\\}$,\n",
    "pooled over many episodes (where the end of an episode occurs when a terminal state is reached) into a replay memory. \n",
    "\n",
    "##### Details\n",
    "During the inner loop of the algorithm,\n",
    "we apply Q-learning updates, or minibatch updates, to samples of experience,\n",
    "$(s, a,r,s^{'}) \\sim U(D)$, drawn at random from the pool of stored samples. This approach\n",
    "has several advantages over standard online Q-learning. First, each step of experience\n",
    "is potentially used in many weight updates, which allowsfor greater data efficiency.\n",
    "\n",
    "Second, learning directly from consecutive samples is inefficient, owing to the strong\n",
    "correlations between the samples; randomizing the samples breaks these correlations and therefore reduces the variance of the updates. Third, when learning onpolicy the current parameters determine the next data sample that the parameters\n",
    "are trained on. For example, if the maximizing action is to move left then the training samples will be dominated by samples from the left-hand side; if the maximizing action then switches to the right then the training distribution will also switch.\n",
    "Itis easy to see how unwanted feedback loops may arise and the parameters could get\n",
    "stuckin a poor localminimum, or even diverge catastrophically. By using experience replay the behaviour distribution is averaged over many of its previous states,\n",
    "smoothing out learning and avoiding oscillations or divergence in the parameters.\n",
    "Note that when learning by experience replay, it is necessary to learn off-policy\n",
    "(because our current parameters are different to those used to generate the sample), which motivates the choice of Q-learning.\n",
    "\n",
    "\n",
    "In practice, our algorithm only stores the last $N$ experience tuples in the replay\n",
    "memory, and samples uniformly at random from $D$ when performing updates. This\n",
    "approach is in some respects limited because the memory buffer does not differentiate important transitions and always overwrites with recent transitions owing\n",
    "to the finite memory size $N$. Similarly, the uniform sampling gives equal importance to all transitions in the replay memory. A more sophisticated sampling strategy might emphasize transitions from which we can learn the most, similar to\n",
    "prioritized sweeping.\n",
    "\n",
    "\n",
    "\n",
    "#### Tips\n",
    "Store frames in unit8 to save memory \n",
    "\n",
    "state t and state t+1 share 3 frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df493862",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory():\n",
    "    def __init__(self):\n",
    "        self.replay_mem = deque(maxlen=memory_size)\n",
    "        self.epinfos_mem = deque([], maxlen=100)\n",
    "    def update_replay(self,experience):\n",
    "        self.replay_mem.append(experience)\n",
    "        \n",
    "    def update_epinfos(self,info):\n",
    "        self.epinfos_mem.append(info)\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44701fb4",
   "metadata": {},
   "source": [
    "### Architecture\n",
    "Based on https://www.nature.com/articles/nature14236\n",
    "\n",
    "The input to the neural network consists of an 84x84x4 image produced by the preprocessing map w. \n",
    "\n",
    "The first hidden layer convolves 32 filters of 8x8 with stride 4 with the\n",
    "input image and applies a rectifier nonlinearity. \n",
    "\n",
    "The second hidden layer convolves 64 filters of 4x4 with stride 2, again followed by a rectifier nonlinearity.\n",
    "\n",
    "This is followed by a third convolutional layer that convolves 64filters of 3x3 with\n",
    "stride 1 followed by a rectifier. \n",
    "\n",
    "The final hidden layer is fully-connected and consists of 512 rectifier units. The output layer is a fully-connected linear layer with a single output for each valid action. \n",
    "\n",
    "##### We have 4 valid actions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "943acbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQ(nn.Module):\n",
    "    # TODO clean up\n",
    "    def __init__(self,env, device = \"cuda\"):\n",
    "        super().__init__()\n",
    "        self.in_features = env.observation_space.shape[0]\n",
    "        self.action_space = env.action_space.n\n",
    "        self.device = device\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(self.in_features, out_channels = 32, kernel_size = (8,8), stride = 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, (4,4), 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64,64,(3,3),1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(64*7*7,512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,self.action_space)\n",
    "        )\n",
    "        \n",
    "    def forward(self,state):\n",
    "        state = self.conv(state)\n",
    "        state = state.flatten(start_dim=1)\n",
    "        state = self.linear(state)\n",
    "        return state\n",
    "        \n",
    "    \n",
    "    def action(self,states,epsilon):\n",
    "        states = torch.as_tensor(states, dtype = torch.float32, device = self.device) # tensor \n",
    "        q_values = self.forward(states) # model output (q_values)\n",
    "        highest_q_indexes = torch.argmax(q_values,dim=1) # find action with highest q value\n",
    "        actions = highest_q_indexes.detach().tolist() # convert tensor to regular number\n",
    "        \n",
    "        for i in range(4):\n",
    "            rnd_sample = random.random()\n",
    "            # overwrite action with random action with epsilon prob, explore\n",
    "            if rnd_sample <= epsilon:\n",
    "                actions[i] = random.randint(0, 3)\n",
    "        return actions\n",
    "    \n",
    "    def loss(self, experiences, target_net):\n",
    "        # divided experience into parts \n",
    "        states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        dones = []\n",
    "        next_states = []\n",
    "        for e in experiences:\n",
    "            states.append(e.state)\n",
    "            actions.append(e.action)\n",
    "            rewards.append(e.reward)\n",
    "            dones.append(e.done)\n",
    "            next_states.append(e.next_state)\n",
    "        \n",
    "        states = np.stack([s.get_frames() for s in states])\n",
    "        next_states =np.stack([s.get_frames() for s in next_states])  \n",
    "        \n",
    "        # convert to tensors\n",
    "        states = torch.as_tensor(states, dtype=torch.float,device = self.device)\n",
    "        actions = torch.as_tensor(np.asarray(actions), dtype=torch.int64,device = self.device).unsqueeze(-1)\n",
    "        rewards = torch.as_tensor(np.asarray(rewards), dtype=torch.float,device = self.device).unsqueeze(-1)\n",
    "        dones = torch.as_tensor(np.asarray(dones), dtype=torch.float,device = self.device).unsqueeze(-1)\n",
    "        next_states = torch.as_tensor(next_states, dtype=torch.float,device = self.device)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # Target\n",
    "        target_q_values = target_network(next_states)\n",
    "        max_target_q_values = target_q_values.max(dim=1, keepdim=True)[0]\n",
    "        # set y_j = r_j + gamma * max(Q_target(next_state)) \n",
    "        # if teminal state y_j = r_j -> (1-done_j) = 0\n",
    "        targets = rewards + gamma * (1 - dones) * max_target_q_values\n",
    "\n",
    "        # Loss\n",
    "        q_values = self.forward(states)\n",
    "        action_q_values = torch.gather(input=q_values, dim=1,index=actions)\n",
    "        loss = nn.functional.mse_loss(action_q_values, targets)\n",
    "        return loss\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fb2273",
   "metadata": {},
   "source": [
    "#### Algorithm 1: deep Q-learning with experience replay\n",
    "source: https://www.nature.com/articles/nature14236/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1511bb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensboard stuff\n",
    "%load_ext tensorboard\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56c7779f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrappers from https://github.com/openai/baselines/blob/master/baselines/common/atari_wrappers.py\n",
    "# some small changes done for compatibility \n",
    "\n",
    "\n",
    "# parallel enviroments\n",
    "env_lambda = lambda: baselines_wrappers.Monitor(make_atari_deepmind(\"BreakoutDeterministic-v4\", scale_values=True), allow_early_resets=True)\n",
    "#vec_env = baselines_wrappers.DummyVecEnv([env_lambda for _ in range(4)])\n",
    "vec_env = baselines_wrappers.SubprocVecEnv([env_lambda for _ in range(4)])\n",
    "# wrapper stacks 4 frames into state \n",
    "env = BatchedPytorchFrameStack(vec_env, k=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3461f68f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Experience= namedtuple(\n",
    "    \"Experience\",\n",
    "    (\"state\", \"action\", \"reward\", \"done\", \"next_state\" )\n",
    ")\n",
    "# initlize memory\n",
    "# Note differences with cartpole version\n",
    "# we have 4 enviroments running seperatley\n",
    "# we therefore we need several actions as inputs\n",
    "# and we get several outputs \n",
    "memory  = ReplayMemory()\n",
    "def mem_init(env,mem):\n",
    "    states = env.reset()\n",
    "    for _ in range(required_mem):\n",
    "        # random uniform actions\n",
    "        actions = [env.action_space.sample() for _ in range(4)]\n",
    "        new_states, rewards, dones, infos = env.step(actions)\n",
    "        for state, action, reward, done, new_state in zip(states, actions, rewards, dones, new_states):\n",
    "            experience = Experience(state,action,reward,done, new_state)\n",
    "            # add to memory\n",
    "            mem.update_replay(experience)\n",
    "            \n",
    "        states = new_states\n",
    "        \n",
    "        # don't need to check done, baselines_wrappers handles it\n",
    "        \n",
    "    return mem\n",
    "\n",
    "memory = mem_init(env,memory)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f26a1f67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6f4888",
   "metadata": {},
   "source": [
    "#### Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c87b8592",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# initlize models\n",
    "action_network = DeepQ(env).to(device)\n",
    "target_network = DeepQ(env).to(device)\n",
    "# init, target = action \n",
    "target_network.load_state_dict(action_network.state_dict())\n",
    "\n",
    "# set optimizer \n",
    "optimizer = torch.optim.Adam(action_network.parameters(), lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d16635a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 84, 84)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b963b68",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    episode_count = 0\n",
    "    states = env.reset()\n",
    "    # while loop with a count\n",
    "    for step in itertools.count():\n",
    "\n",
    "        # get the current epsilon value\n",
    "        epsilon = np.interp(step * 4,[0, e_steps], [e_start, e_end])\n",
    "        # sample from [0.0,...,1.0)\n",
    "        rnd = random.random()\n",
    "        \n",
    "        # action from network, (network handles random actions)\n",
    "        act_states = np.stack([o.get_frames() for o in states])\n",
    "        actions = action_network.action(act_states, epsilon)    \n",
    "            \n",
    "            \n",
    "        new_states, rewards, dones, infos = env.step(actions)\n",
    "        for state, action, reward, done, new_state, info in zip(states, actions, rewards, dones, new_states, infos):\n",
    "            experience = Experience(state,action,reward,done, new_state)\n",
    "            # add to memory\n",
    "            memory.update_replay(experience)\n",
    "            if done:\n",
    "                memory.update_epinfos(info[\"episode\"])\n",
    "                episode_count +=1\n",
    "            \n",
    "        states = new_states\n",
    "\n",
    "\n",
    "        # sample from replay memory (batch size number of experiences)\n",
    "        experiences = random.sample(memory.replay_mem, batch_size)\n",
    "        loss = action_network.loss(experiences,target_network)\n",
    "\n",
    "\n",
    "        # Gradient descent\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # -  every target_update step (called C in paper)\n",
    "        # - save model parameters\n",
    "        # - logg metrics\n",
    "        if step % target_update == 0:\n",
    "            # update target network\n",
    "            target_network.load_state_dict(action_network.state_dict())\n",
    "\n",
    "        # save model\n",
    "        if step % 10000 == 0 and step != 0:\n",
    "            print(\"save\")\n",
    "            torch.save({\n",
    "                'step': step,\n",
    "                'model_state_dict': action_network.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': loss\n",
    "                }, \"./models/Breakout\")\n",
    "                \n",
    "        \n",
    "        # loggs\n",
    "        if step % 10000 ==0:\n",
    "            rew_mean = np.nan_to_num(np.mean([e[\"r\"] for e in memory.epinfos_mem]))\n",
    "            len_mean = np.nan_to_num(np.mean([e[\"l\"] for e in memory.epinfos_mem]))\n",
    "            print()\n",
    "            print(\"Step\", step)\n",
    "            print(\"Avg Reward\", rew_mean)\n",
    "            print(\"Avg Ep Len\", len_mean)\n",
    "            print(\"Episodes\", episode_count)\n",
    "            \n",
    "            writer.add_scalar(\"AvgRew\", rew_mean, global_step = step)\n",
    "            writer.add_scalar(\"AvgEpLen\", len_mean, global_step = step)\n",
    "            writer.add_scalar(\"Episodes\", episode_count, global_step = step)\n",
    "            #step_logging.append(step)\n",
    "            #reward_logging.append(avg_reward)\n",
    "            #loss_logging.append(loss)\n",
    "            #print(\"\\nReward: \", avg_reward)\n",
    "    \n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b3e5f77",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emanu\\anaconda3\\envs\\dmlgpu\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\emanu\\anaconda3\\envs\\dmlgpu\\lib\\site-packages\\numpy\\core\\_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 0\n",
      "Avg Reward 0.0\n",
      "Avg Ep Len 0.0\n",
      "Episodes 0\n",
      "save\n",
      "\n",
      "Step 10000\n",
      "Avg Reward 0.18\n",
      "Avg Ep Len 34.21\n",
      "Episodes 1128\n",
      "save\n",
      "\n",
      "Step 20000\n",
      "Avg Reward 0.31\n",
      "Avg Ep Len 39.95\n",
      "Episodes 2248\n",
      "save\n",
      "\n",
      "Step 30000\n",
      "Avg Reward 0.17\n",
      "Avg Ep Len 33.15\n",
      "Episodes 3389\n",
      "save\n",
      "\n",
      "Step 40000\n",
      "Avg Reward 0.25\n",
      "Avg Ep Len 36.96\n",
      "Episodes 4523\n",
      "save\n",
      "\n",
      "Step 50000\n",
      "Avg Reward 0.2\n",
      "Avg Ep Len 34.04\n",
      "Episodes 5612\n",
      "save\n",
      "\n",
      "Step 60000\n",
      "Avg Reward 0.4\n",
      "Avg Ep Len 41.1\n",
      "Episodes 6584\n",
      "save\n",
      "\n",
      "Step 70000\n",
      "Avg Reward 0.42\n",
      "Avg Ep Len 42.47\n",
      "Episodes 7536\n",
      "save\n",
      "\n",
      "Step 80000\n",
      "Avg Reward 0.47\n",
      "Avg Ep Len 42.96\n",
      "Episodes 8405\n",
      "save\n",
      "\n",
      "Step 90000\n",
      "Avg Reward 0.74\n",
      "Avg Ep Len 55.2\n",
      "Episodes 9192\n",
      "save\n",
      "\n",
      "Step 100000\n",
      "Avg Reward 0.51\n",
      "Avg Ep Len 46.47\n",
      "Episodes 10068\n",
      "save\n",
      "\n",
      "Step 110000\n",
      "Avg Reward 0.88\n",
      "Avg Ep Len 61.86\n",
      "Episodes 10891\n",
      "save\n",
      "\n",
      "Step 120000\n",
      "Avg Reward 0.89\n",
      "Avg Ep Len 59.07\n",
      "Episodes 11580\n",
      "save\n",
      "\n",
      "Step 130000\n",
      "Avg Reward 0.98\n",
      "Avg Ep Len 64.24\n",
      "Episodes 12179\n",
      "save\n",
      "\n",
      "Step 140000\n",
      "Avg Reward 1.34\n",
      "Avg Ep Len 76.06\n",
      "Episodes 12738\n",
      "save\n",
      "\n",
      "Step 150000\n",
      "Avg Reward 1.47\n",
      "Avg Ep Len 79.68\n",
      "Episodes 13234\n",
      "save\n",
      "\n",
      "Step 160000\n",
      "Avg Reward 1.91\n",
      "Avg Ep Len 93.47\n",
      "Episodes 13682\n",
      "save\n",
      "\n",
      "Step 170000\n",
      "Avg Reward 2.09\n",
      "Avg Ep Len 98.94\n",
      "Episodes 14112\n",
      "save\n",
      "\n",
      "Step 180000\n",
      "Avg Reward 2.91\n",
      "Avg Ep Len 126.18\n",
      "Episodes 14465\n",
      "save\n",
      "\n",
      "Step 190000\n",
      "Avg Reward 3.5\n",
      "Avg Ep Len 146.95\n",
      "Episodes 14783\n",
      "save\n",
      "\n",
      "Step 200000\n",
      "Avg Reward 3.29\n",
      "Avg Ep Len 137.59\n",
      "Episodes 15109\n",
      "save\n",
      "\n",
      "Step 210000\n",
      "Avg Reward 4.18\n",
      "Avg Ep Len 165.51\n",
      "Episodes 15362\n",
      "save\n",
      "\n",
      "Step 220000\n",
      "Avg Reward 4.08\n",
      "Avg Ep Len 161.19\n",
      "Episodes 15631\n",
      "save\n",
      "\n",
      "Step 230000\n",
      "Avg Reward 4.25\n",
      "Avg Ep Len 167.55\n",
      "Episodes 15869\n",
      "save\n",
      "\n",
      "Step 240000\n",
      "Avg Reward 5.22\n",
      "Avg Ep Len 200.16\n",
      "Episodes 16073\n",
      "save\n",
      "\n",
      "Step 250000\n",
      "Avg Reward 5.55\n",
      "Avg Ep Len 206.13\n",
      "Episodes 16273\n",
      "save\n",
      "\n",
      "Step 260000\n",
      "Avg Reward 6.65\n",
      "Avg Ep Len 245.32\n",
      "Episodes 16445\n",
      "save\n",
      "\n",
      "Step 270000\n",
      "Avg Reward 6.39\n",
      "Avg Ep Len 242.61\n",
      "Episodes 16611\n",
      "save\n",
      "\n",
      "Step 280000\n",
      "Avg Reward 6.17\n",
      "Avg Ep Len 225.4\n",
      "Episodes 16785\n",
      "save\n",
      "\n",
      "Step 290000\n",
      "Avg Reward 6.49\n",
      "Avg Ep Len 240.3\n",
      "Episodes 16953\n",
      "save\n",
      "\n",
      "Step 300000\n",
      "Avg Reward 6.16\n",
      "Avg Ep Len 229.57\n",
      "Episodes 17124\n",
      "save\n",
      "\n",
      "Step 310000\n",
      "Avg Reward 6.55\n",
      "Avg Ep Len 240.45\n",
      "Episodes 17298\n",
      "save\n",
      "\n",
      "Step 320000\n",
      "Avg Reward 6.94\n",
      "Avg Ep Len 248.46\n",
      "Episodes 17465\n",
      "save\n",
      "\n",
      "Step 330000\n",
      "Avg Reward 7.23\n",
      "Avg Ep Len 260.03\n",
      "Episodes 17628\n",
      "save\n",
      "\n",
      "Step 340000\n",
      "Avg Reward 7.6\n",
      "Avg Ep Len 268.54\n",
      "Episodes 17785\n",
      "save\n",
      "\n",
      "Step 350000\n",
      "Avg Reward 7.27\n",
      "Avg Ep Len 254.88\n",
      "Episodes 17939\n",
      "save\n",
      "\n",
      "Step 360000\n",
      "Avg Reward 7.43\n",
      "Avg Ep Len 261.18\n",
      "Episodes 18090\n",
      "save\n",
      "\n",
      "Step 370000\n",
      "Avg Reward 7.94\n",
      "Avg Ep Len 271.57\n",
      "Episodes 18241\n",
      "save\n",
      "\n",
      "Step 380000\n",
      "Avg Reward 7.67\n",
      "Avg Ep Len 251.49\n",
      "Episodes 18391\n",
      "save\n",
      "\n",
      "Step 390000\n",
      "Avg Reward 7.99\n",
      "Avg Ep Len 265.95\n",
      "Episodes 18542\n",
      "save\n",
      "\n",
      "Step 400000\n",
      "Avg Reward 8.16\n",
      "Avg Ep Len 273.04\n",
      "Episodes 18695\n",
      "save\n",
      "\n",
      "Step 410000\n",
      "Avg Reward 7.31\n",
      "Avg Ep Len 243.13\n",
      "Episodes 18849\n",
      "save\n",
      "\n",
      "Step 420000\n",
      "Avg Reward 7.26\n",
      "Avg Ep Len 253.42\n",
      "Episodes 19003\n",
      "save\n",
      "\n",
      "Step 430000\n",
      "Avg Reward 7.03\n",
      "Avg Ep Len 240.59\n",
      "Episodes 19159\n",
      "save\n",
      "\n",
      "Step 440000\n",
      "Avg Reward 7.14\n",
      "Avg Ep Len 238.24\n",
      "Episodes 19326\n",
      "save\n",
      "\n",
      "Step 450000\n",
      "Avg Reward 6.79\n",
      "Avg Ep Len 231.31\n",
      "Episodes 19494\n",
      "save\n",
      "\n",
      "Step 460000\n",
      "Avg Reward 8.3\n",
      "Avg Ep Len 267.78\n",
      "Episodes 19654\n",
      "save\n",
      "\n",
      "Step 470000\n",
      "Avg Reward 6.98\n",
      "Avg Ep Len 240.28\n",
      "Episodes 19814\n",
      "save\n",
      "\n",
      "Step 480000\n",
      "Avg Reward 7.51\n",
      "Avg Ep Len 253.08\n",
      "Episodes 19974\n",
      "save\n",
      "\n",
      "Step 490000\n",
      "Avg Reward 7.12\n",
      "Avg Ep Len 248.29\n",
      "Episodes 20139\n",
      "save\n",
      "\n",
      "Step 500000\n",
      "Avg Reward 7.67\n",
      "Avg Ep Len 257.89\n",
      "Episodes 20291\n",
      "save\n",
      "\n",
      "Step 510000\n",
      "Avg Reward 8.11\n",
      "Avg Ep Len 264.84\n",
      "Episodes 20450\n",
      "save\n",
      "\n",
      "Step 520000\n",
      "Avg Reward 7.58\n",
      "Avg Ep Len 250.96\n",
      "Episodes 20610\n",
      "save\n",
      "\n",
      "Step 530000\n",
      "Avg Reward 6.44\n",
      "Avg Ep Len 232.25\n",
      "Episodes 20783\n",
      "save\n",
      "\n",
      "Step 540000\n",
      "Avg Reward 7.79\n",
      "Avg Ep Len 263.3\n",
      "Episodes 20942\n",
      "save\n",
      "\n",
      "Step 550000\n",
      "Avg Reward 8.26\n",
      "Avg Ep Len 268.3\n",
      "Episodes 21088\n",
      "save\n",
      "\n",
      "Step 560000\n",
      "Avg Reward 8.26\n",
      "Avg Ep Len 275.21\n",
      "Episodes 21233\n",
      "save\n",
      "\n",
      "Step 570000\n",
      "Avg Reward 9.45\n",
      "Avg Ep Len 291.76\n",
      "Episodes 21376\n",
      "save\n",
      "\n",
      "Step 580000\n",
      "Avg Reward 7.82\n",
      "Avg Ep Len 255.43\n",
      "Episodes 21525\n",
      "save\n",
      "\n",
      "Step 590000\n",
      "Avg Reward 6.82\n",
      "Avg Ep Len 238.23\n",
      "Episodes 21696\n",
      "save\n",
      "\n",
      "Step 600000\n",
      "Avg Reward 7.18\n",
      "Avg Ep Len 246.13\n",
      "Episodes 21852\n",
      "save\n",
      "\n",
      "Step 610000\n",
      "Avg Reward 7.94\n",
      "Avg Ep Len 260.21\n",
      "Episodes 22005\n",
      "save\n",
      "\n",
      "Step 620000\n",
      "Avg Reward 7.88\n",
      "Avg Ep Len 261.13\n",
      "Episodes 22157\n",
      "save\n",
      "\n",
      "Step 630000\n",
      "Avg Reward 7.99\n",
      "Avg Ep Len 265.35\n",
      "Episodes 22313\n",
      "save\n",
      "\n",
      "Step 640000\n",
      "Avg Reward 6.37\n",
      "Avg Ep Len 227.59\n",
      "Episodes 22479\n",
      "save\n",
      "\n",
      "Step 650000\n",
      "Avg Reward 4.89\n",
      "Avg Ep Len 185.33\n",
      "Episodes 22666\n",
      "save\n",
      "\n",
      "Step 660000\n",
      "Avg Reward 4.75\n",
      "Avg Ep Len 182.55\n",
      "Episodes 22869\n",
      "save\n",
      "\n",
      "Step 670000\n",
      "Avg Reward 4.83\n",
      "Avg Ep Len 183.11\n",
      "Episodes 23086\n",
      "save\n",
      "\n",
      "Step 680000\n",
      "Avg Reward 2.83\n",
      "Avg Ep Len 123.06\n",
      "Episodes 23384\n",
      "save\n",
      "\n",
      "Step 690000\n",
      "Avg Reward 1.39\n",
      "Avg Ep Len 76.95\n",
      "Episodes 23795\n",
      "save\n",
      "\n",
      "Step 700000\n",
      "Avg Reward 1.36\n",
      "Avg Ep Len 75.66\n",
      "Episodes 24240\n",
      "save\n",
      "\n",
      "Step 710000\n",
      "Avg Reward 0.82\n",
      "Avg Ep Len 57.76\n",
      "Episodes 24847\n",
      "save\n",
      "\n",
      "Step 720000\n",
      "Avg Reward 2.6\n",
      "Avg Ep Len 116.38\n",
      "Episodes 25246\n",
      "save\n",
      "\n",
      "Step 730000\n",
      "Avg Reward 3.01\n",
      "Avg Ep Len 131.47\n",
      "Episodes 25569\n",
      "save\n",
      "\n",
      "Step 740000\n",
      "Avg Reward 3.11\n",
      "Avg Ep Len 133.7\n",
      "Episodes 25868\n",
      "save\n",
      "\n",
      "Step 750000\n",
      "Avg Reward 3.67\n",
      "Avg Ep Len 148.94\n",
      "Episodes 26136\n",
      "save\n",
      "\n",
      "Step 760000\n",
      "Avg Reward 5.55\n",
      "Avg Ep Len 208.0\n",
      "Episodes 26387\n",
      "save\n",
      "\n",
      "Step 770000\n",
      "Avg Reward 5.07\n",
      "Avg Ep Len 190.05\n",
      "Episodes 26594\n",
      "save\n",
      "\n",
      "Step 780000\n",
      "Avg Reward 5.17\n",
      "Avg Ep Len 192.14\n",
      "Episodes 26790\n",
      "save\n",
      "\n",
      "Step 790000\n",
      "Avg Reward 5.92\n",
      "Avg Ep Len 227.33\n",
      "Episodes 26979\n",
      "save\n",
      "\n",
      "Step 800000\n",
      "Avg Reward 4.37\n",
      "Avg Ep Len 170.3\n",
      "Episodes 27199\n",
      "save\n",
      "\n",
      "Step 810000\n",
      "Avg Reward 4.3\n",
      "Avg Ep Len 171.35\n",
      "Episodes 27415\n",
      "save\n",
      "\n",
      "Step 820000\n",
      "Avg Reward 4.57\n",
      "Avg Ep Len 179.75\n",
      "Episodes 27631\n",
      "save\n",
      "\n",
      "Step 830000\n",
      "Avg Reward 4.82\n",
      "Avg Ep Len 186.25\n",
      "Episodes 27847\n",
      "save\n",
      "\n",
      "Step 840000\n",
      "Avg Reward 5.14\n",
      "Avg Ep Len 200.97\n",
      "Episodes 28059\n",
      "save\n",
      "\n",
      "Step 850000\n",
      "Avg Reward 5.1\n",
      "Avg Ep Len 195.61\n",
      "Episodes 28270\n",
      "save\n",
      "\n",
      "Step 860000\n",
      "Avg Reward 3.85\n",
      "Avg Ep Len 158.73\n",
      "Episodes 28538\n",
      "save\n",
      "\n",
      "Step 870000\n",
      "Avg Reward 4.24\n",
      "Avg Ep Len 175.96\n",
      "Episodes 28750\n",
      "save\n",
      "\n",
      "Step 880000\n",
      "Avg Reward 3.91\n",
      "Avg Ep Len 160.89\n",
      "Episodes 28981\n",
      "save\n",
      "\n",
      "Step 890000\n",
      "Avg Reward 5.58\n",
      "Avg Ep Len 214.73\n",
      "Episodes 29171\n",
      "save\n",
      "\n",
      "Step 900000\n",
      "Avg Reward 7.34\n",
      "Avg Ep Len 258.71\n",
      "Episodes 29332\n",
      "save\n",
      "\n",
      "Step 910000\n",
      "Avg Reward 7.53\n",
      "Avg Ep Len 256.18\n",
      "Episodes 29483\n",
      "save\n",
      "\n",
      "Step 920000\n",
      "Avg Reward 8.62\n",
      "Avg Ep Len 296.8\n",
      "Episodes 29621\n",
      "save\n",
      "\n",
      "Step 930000\n",
      "Avg Reward 7.64\n",
      "Avg Ep Len 260.0\n",
      "Episodes 29771\n",
      "save\n",
      "\n",
      "Step 940000\n",
      "Avg Reward 9.52\n",
      "Avg Ep Len 297.63\n",
      "Episodes 29908\n",
      "save\n",
      "\n",
      "Step 950000\n",
      "Avg Reward 9.29\n",
      "Avg Ep Len 286.09\n",
      "Episodes 30053\n",
      "save\n",
      "\n",
      "Step 960000\n",
      "Avg Reward 8.28\n",
      "Avg Ep Len 266.9\n",
      "Episodes 30203\n",
      "save\n",
      "\n",
      "Step 970000\n",
      "Avg Reward 8.75\n",
      "Avg Ep Len 283.72\n",
      "Episodes 30338\n",
      "save\n",
      "\n",
      "Step 980000\n",
      "Avg Reward 9.06\n",
      "Avg Ep Len 276.2\n",
      "Episodes 30483\n",
      "save\n",
      "\n",
      "Step 990000\n",
      "Avg Reward 8.61\n",
      "Avg Ep Len 264.31\n",
      "Episodes 30627\n",
      "save\n",
      "\n",
      "Step 1000000\n",
      "Avg Reward 8.97\n",
      "Avg Ep Len 272.99\n",
      "Episodes 30768\n",
      "save\n",
      "\n",
      "Step 1010000\n",
      "Avg Reward 8.82\n",
      "Avg Ep Len 293.78\n",
      "Episodes 30915\n",
      "save\n",
      "\n",
      "Step 1020000\n",
      "Avg Reward 10.23\n",
      "Avg Ep Len 302.72\n",
      "Episodes 31053\n",
      "save\n",
      "\n",
      "Step 1030000\n",
      "Avg Reward 8.92\n",
      "Avg Ep Len 274.99\n",
      "Episodes 31197\n",
      "save\n",
      "\n",
      "Step 1040000\n",
      "Avg Reward 8.25\n",
      "Avg Ep Len 260.27\n",
      "Episodes 31348\n",
      "save\n",
      "\n",
      "Step 1050000\n",
      "Avg Reward 10.2\n",
      "Avg Ep Len 301.26\n",
      "Episodes 31486\n",
      "save\n",
      "\n",
      "Step 1060000\n",
      "Avg Reward 10.49\n",
      "Avg Ep Len 313.39\n",
      "Episodes 31620\n",
      "save\n",
      "\n",
      "Step 1070000\n",
      "Avg Reward 8.22\n",
      "Avg Ep Len 257.32\n",
      "Episodes 31769\n",
      "save\n",
      "\n",
      "Step 1080000\n",
      "Avg Reward 8.91\n",
      "Avg Ep Len 284.22\n",
      "Episodes 31909\n",
      "save\n",
      "\n",
      "Step 1090000\n",
      "Avg Reward 8.65\n",
      "Avg Ep Len 270.26\n",
      "Episodes 32056\n",
      "save\n",
      "\n",
      "Step 1100000\n",
      "Avg Reward 8.24\n",
      "Avg Ep Len 236.35\n",
      "Episodes 32221\n",
      "save\n",
      "\n",
      "Step 1110000\n",
      "Avg Reward 9.34\n",
      "Avg Ep Len 294.51\n",
      "Episodes 32357\n",
      "save\n",
      "\n",
      "Step 1120000\n",
      "Avg Reward 9.64\n",
      "Avg Ep Len 294.93\n",
      "Episodes 32503\n",
      "save\n",
      "\n",
      "Step 1130000\n",
      "Avg Reward 9.09\n",
      "Avg Ep Len 281.89\n",
      "Episodes 32648\n",
      "save\n",
      "\n",
      "Step 1140000\n",
      "Avg Reward 7.89\n",
      "Avg Ep Len 251.21\n",
      "Episodes 32808\n",
      "save\n",
      "\n",
      "Step 1150000\n",
      "Avg Reward 5.45\n",
      "Avg Ep Len 197.51\n",
      "Episodes 32977\n",
      "save\n",
      "\n",
      "Step 1160000\n",
      "Avg Reward 8.02\n",
      "Avg Ep Len 253.8\n",
      "Episodes 33128\n",
      "save\n",
      "\n",
      "Step 1170000\n",
      "Avg Reward 7.46\n",
      "Avg Ep Len 244.33\n",
      "Episodes 33299\n",
      "save\n",
      "\n",
      "Step 1180000\n",
      "Avg Reward 7.94\n",
      "Avg Ep Len 258.55\n",
      "Episodes 33455\n",
      "save\n",
      "\n",
      "Step 1190000\n",
      "Avg Reward 7.85\n",
      "Avg Ep Len 260.91\n",
      "Episodes 33608\n",
      "save\n",
      "\n",
      "Step 1200000\n",
      "Avg Reward 8.94\n",
      "Avg Ep Len 282.93\n",
      "Episodes 33752\n",
      "save\n",
      "\n",
      "Step 1210000\n",
      "Avg Reward 8.33\n",
      "Avg Ep Len 263.23\n",
      "Episodes 33908\n",
      "save\n",
      "\n",
      "Step 1220000\n",
      "Avg Reward 9.5\n",
      "Avg Ep Len 276.39\n",
      "Episodes 34055\n",
      "save\n",
      "\n",
      "Step 1230000\n",
      "Avg Reward 8.17\n",
      "Avg Ep Len 262.86\n",
      "Episodes 34210\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save\n",
      "\n",
      "Step 1240000\n",
      "Avg Reward 8.41\n",
      "Avg Ep Len 264.84\n",
      "Episodes 34360\n",
      "save\n",
      "\n",
      "Step 1250000\n",
      "Avg Reward 7.98\n",
      "Avg Ep Len 260.84\n",
      "Episodes 34513\n",
      "save\n",
      "\n",
      "Step 1260000\n",
      "Avg Reward 10.0\n",
      "Avg Ep Len 290.33\n",
      "Episodes 34666\n",
      "save\n",
      "\n",
      "Step 1270000\n",
      "Avg Reward 11.07\n",
      "Avg Ep Len 299.71\n",
      "Episodes 34803\n",
      "save\n",
      "\n",
      "Step 1280000\n",
      "Avg Reward 10.21\n",
      "Avg Ep Len 302.28\n",
      "Episodes 34937\n",
      "save\n",
      "\n",
      "Step 1290000\n",
      "Avg Reward 8.9\n",
      "Avg Ep Len 269.21\n",
      "Episodes 35080\n",
      "save\n",
      "\n",
      "Step 1300000\n",
      "Avg Reward 9.64\n",
      "Avg Ep Len 263.71\n",
      "Episodes 35226\n",
      "save\n",
      "\n",
      "Step 1310000\n",
      "Avg Reward 8.1\n",
      "Avg Ep Len 252.73\n",
      "Episodes 35380\n",
      "save\n",
      "\n",
      "Step 1320000\n",
      "Avg Reward 8.94\n",
      "Avg Ep Len 271.7\n",
      "Episodes 35525\n",
      "save\n",
      "\n",
      "Step 1330000\n",
      "Avg Reward 8.31\n",
      "Avg Ep Len 270.35\n",
      "Episodes 35682\n",
      "save\n",
      "\n",
      "Step 1340000\n",
      "Avg Reward 9.06\n",
      "Avg Ep Len 286.04\n",
      "Episodes 35823\n",
      "save\n",
      "\n",
      "Step 1350000\n",
      "Avg Reward 8.29\n",
      "Avg Ep Len 251.01\n",
      "Episodes 35986\n",
      "save\n",
      "\n",
      "Step 1360000\n",
      "Avg Reward 8.3\n",
      "Avg Ep Len 266.1\n",
      "Episodes 36149\n",
      "save\n",
      "\n",
      "Step 1370000\n",
      "Avg Reward 8.78\n",
      "Avg Ep Len 279.2\n",
      "Episodes 36297\n",
      "save\n",
      "\n",
      "Step 1380000\n",
      "Avg Reward 11.29\n",
      "Avg Ep Len 313.25\n",
      "Episodes 36426\n",
      "save\n",
      "\n",
      "Step 1390000\n",
      "Avg Reward 9.66\n",
      "Avg Ep Len 282.42\n",
      "Episodes 36563\n",
      "save\n",
      "\n",
      "Step 1400000\n",
      "Avg Reward 10.5\n",
      "Avg Ep Len 313.6\n",
      "Episodes 36699\n",
      "save\n",
      "\n",
      "Step 1410000\n",
      "Avg Reward 10.0\n",
      "Avg Ep Len 281.27\n",
      "Episodes 36840\n",
      "save\n",
      "\n",
      "Step 1420000\n",
      "Avg Reward 9.7\n",
      "Avg Ep Len 282.19\n",
      "Episodes 36979\n",
      "save\n",
      "\n",
      "Step 1430000\n",
      "Avg Reward 8.39\n",
      "Avg Ep Len 269.61\n",
      "Episodes 37128\n",
      "save\n",
      "\n",
      "Step 1440000\n",
      "Avg Reward 9.0\n",
      "Avg Ep Len 265.89\n",
      "Episodes 37272\n",
      "save\n",
      "\n",
      "Step 1450000\n",
      "Avg Reward 10.68\n",
      "Avg Ep Len 312.47\n",
      "Episodes 37402\n",
      "save\n",
      "\n",
      "Step 1460000\n",
      "Avg Reward 9.39\n",
      "Avg Ep Len 280.01\n",
      "Episodes 37543\n",
      "save\n",
      "\n",
      "Step 1470000\n",
      "Avg Reward 9.74\n",
      "Avg Ep Len 278.43\n",
      "Episodes 37688\n",
      "save\n",
      "\n",
      "Step 1480000\n",
      "Avg Reward 9.44\n",
      "Avg Ep Len 288.27\n",
      "Episodes 37829\n",
      "save\n",
      "\n",
      "Step 1490000\n",
      "Avg Reward 11.39\n",
      "Avg Ep Len 310.93\n",
      "Episodes 37956\n",
      "save\n",
      "\n",
      "Step 1500000\n",
      "Avg Reward 7.92\n",
      "Avg Ep Len 262.81\n",
      "Episodes 38102\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [13]\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Gradient descent\u001b[39;00m\n\u001b[0;32m     38\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 39\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# -  every target_update step (called C in paper)\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# - save model parameters\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# - logg metrics\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dmlgpu\\lib\\site-packages\\torch\\_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    355\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    356\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    357\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    361\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[0;32m    362\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[1;32m--> 363\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dmlgpu\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f1eb8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acd18bc",
   "metadata": {},
   "source": [
    "#### Display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "70b32ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DeepQ(env_display).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=25e-5)\n",
    "\n",
    "state_of_model = torch.load(\"./models/Breakout\")\n",
    "model.load_state_dict(state_of_model[\"model_state_dict\"])\n",
    "\n",
    "optimizer.load_state_dict(state_of_model['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "88aafafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "env_lambda_display = lambda: baselines_wrappers.Monitor(make_atari_deepmind(\"BreakoutDeterministic-v4\", scale_values=True), allow_early_resets=True)\n",
    "vec_env_display = baselines_wrappers.DummyVecEnv([env_lambda_display for _ in range(1)])\n",
    "env_display = BatchedPytorchFrameStack(vec_env_display, k=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c07fdcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env_display.reset()\n",
    "episode_reward = 0.0\n",
    "episodes_reward =[]\n",
    "episode_count = 0\n",
    "while episode_count < 5:\n",
    "    \n",
    "    # stack frames to state\n",
    "    act_state = np.stack([s.get_frames() for s in state])\n",
    "    # no random action epsilon = 0\n",
    "    action = model.action(act_state, 0.0)\n",
    "    \n",
    "    if first_eps:\n",
    "        # start with firing ball\n",
    "        # models sometimes gets stuck otherwise\n",
    "        action = [1]\n",
    "        first_eps = False \n",
    "    state, reward, done, _ = env_display.step(action)\n",
    "    episode_reward += reward\n",
    "    \n",
    "    if done[0]:\n",
    "        episode_count += 1\n",
    "        episodes_reward.append(episode_reward)\n",
    "        episode_reward = 0.0\n",
    "        state = env_display.reset()\n",
    "        first_eps = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "304c2e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean episode reward:  16.2\n",
      "[array([33.], dtype=float32), array([26.], dtype=float32), array([1.], dtype=float32), array([21.], dtype=float32), array([0.], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean episode reward: \", np.mean(episodes_reward))\n",
    "print(episodes_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c61f7a4",
   "metadata": {},
   "source": [
    "#### Random Move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5878fa63",
   "metadata": {},
   "outputs": [],
   "source": [
    "enviroment = gym.make(\"BreakoutDeterministic-v4\", render_mode=\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ad5c8f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean episode reward:  0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "enviroment.reset()\n",
    "terminated = False\n",
    "last_img = None\n",
    "episode_reward = 0.0\n",
    "episodes_reward =[]\n",
    "for i in range(1000):\n",
    "    action = enviroment.action_space.sample()\n",
    "    observation, reward, done, _ = enviroment.step(action)\n",
    "    episode_reward += reward\n",
    "    if done:\n",
    "        episodes_reward.append(episode_reward)\n",
    "        episode_reward = 0.0\n",
    "        enviroment.reset()\n",
    "        \n",
    "    \n",
    "print(\"Mean episode reward: \", np.mean(episodes_reward))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "45dc6664",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 0.0, 1.0, 1.0, 1.0, 0.0]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "episodes_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d84008",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
