{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f88bf2dd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f88bf2dd",
        "outputId": "d78570d8-d53e-4b61-c704-6fa29ed54e34"
      },
      "outputs": [],
      "source": [
        "#!pip install gym \n",
        "#!pip install ale-py \n",
        "#!pip install gym[atari,accept-rom-license]==0.21.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "a5841e9d",
      "metadata": {
        "id": "a5841e9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "False\n",
            "0\n"
          ]
        },
        {
          "ename": "AssertionError",
          "evalue": "Torch not compiled with CUDA enabled",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[1;32m/Users/alvinanestrand/Documents/GitHub/DAT410-project-DQ-breakout/DesignAImini_project.ipynb Cell 2\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alvinanestrand/Documents/GitHub/DAT410-project-DQ-breakout/DesignAImini_project.ipynb#W1sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mprint\u001b[39m(torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available())\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alvinanestrand/Documents/GitHub/DAT410-project-DQ-breakout/DesignAImini_project.ipynb#W1sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mprint\u001b[39m(torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mdevice_count())\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/alvinanestrand/Documents/GitHub/DAT410-project-DQ-breakout/DesignAImini_project.ipynb#W1sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mprint\u001b[39m(torch\u001b[39m.\u001b[39;49mcuda\u001b[39m.\u001b[39;49mcurrent_device())\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alvinanestrand/Documents/GitHub/DAT410-project-DQ-breakout/DesignAImini_project.ipynb#W1sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mprint\u001b[39m(torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mget_device_name(\u001b[39m0\u001b[39m))\n",
            "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/cuda/__init__.py:481\u001b[0m, in \u001b[0;36mcurrent_device\u001b[0;34m()\u001b[0m\n\u001b[1;32m    479\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcurrent_device\u001b[39m() \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mint\u001b[39m:\n\u001b[1;32m    480\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"Returns the index of a currently selected device.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 481\u001b[0m     _lazy_init()\n\u001b[1;32m    482\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_cuda_getDevice()\n",
            "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/cuda/__init__.py:210\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    207\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    208\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmultiprocessing, you must use the \u001b[39m\u001b[39m'\u001b[39m\u001b[39mspawn\u001b[39m\u001b[39m'\u001b[39m\u001b[39m start method\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    209\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(torch\u001b[39m.\u001b[39m_C, \u001b[39m'\u001b[39m\u001b[39m_cuda_getDeviceCount\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m--> 210\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTorch not compiled with CUDA enabled\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    211\u001b[0m \u001b[39mif\u001b[39;00m _cudart \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    212\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\n\u001b[1;32m    213\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image \n",
        "from tqdm.notebook import tqdm\n",
        "import random\n",
        "import gym\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "import torchvision.transforms as T\n",
        "\n",
        "\n",
        "\n",
        "# set device\n",
        "# test gpu working \n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(torch.cuda.is_available())\n",
        "print(torch.cuda.device_count())\n",
        "print(torch.cuda.current_device())\n",
        "print(torch.cuda.get_device_name(0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "ffe06ddd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "id": "ffe06ddd",
        "outputId": "3cc1557f-9427-4daf-f0bf-45681fb7ba7e"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-c96b94aeeb28>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Breakout-v4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# BreakoutDeterministic-v4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0menviroment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ALE/Breakout-v5\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrender_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"rgb_array\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'gym' is not defined"
          ]
        }
      ],
      "source": [
        "# ALE/Breakout-v5\n",
        "# Breakout-v4 \n",
        "# BreakoutDeterministic-v4\n",
        "env = gym.make(\"ALE/Breakout-v5\", render_mode=\"rgb_array\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76003fe4",
      "metadata": {
        "id": "76003fe4"
      },
      "outputs": [],
      "source": [
        "env.reset()\n",
        "terminated = False\n",
        "last_img = None\n",
        "while not terminated:\n",
        "    env.render(mode=\"rgb_array\")\n",
        "    action = env.action_space.sample()\n",
        "    observation, reward, terminated, info = env.step(action) # truncated is not returned here, probably a version thing\n",
        "    last_img = observation\n",
        "    if terminated:\n",
        "       env.reset()\n",
        "    \n",
        "env.reset()\n",
        "env.close()\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ef54cab",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        },
        "id": "6ef54cab",
        "outputId": "4504f015-2e3e-4be4-e65a-43618cc9cbd1",
        "scrolled": true
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.imshow(last_img)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93ee8301",
      "metadata": {
        "id": "93ee8301"
      },
      "source": [
        "## Preprocessing\n",
        "\n",
        "84x84x1 as used in paper \n",
        "https://arxiv.org/pdf/1312.5602.pdf \n",
        "\n",
        "4.1 Preprocessing and Model Architecture\n",
        "\n",
        "The raw frames are preprocessed by first converting their RGB representation\n",
        "to gray-scale and down-sampling it to a 110×84 image.\n",
        "\n",
        "The final input representation is obtained by\n",
        "cropping an 84 × 84 region of the image that roughly captures the playing area\n",
        "\n",
        " note: tensor has channel as first dimension"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "587ae6d5",
      "metadata": {
        "id": "587ae6d5"
      },
      "outputs": [],
      "source": [
        "from torchvision.transforms import Compose, ToTensor, Normalize, Grayscale, Resize\n",
        "# No samples for calculate mean std for normalisaiton - skip\n",
        "class Preprocess():\n",
        "    def __init__(self, target_width = 84, target_height = 84):\n",
        "        self.width = target_width\n",
        "        self.height = target_height\n",
        "        \n",
        "        # transform\n",
        "        self.transform = T.Compose([\n",
        "            ToTensor(),\n",
        "            Grayscale(),\n",
        "            Resize((110,84))\n",
        "        ])\n",
        "    \n",
        "    def process(self,image):\n",
        "        #return self.transform(image)\n",
        "        # crop to play area not clear from paper exactly which area is considered\n",
        "        # tested different values\n",
        "        return T.functional.crop(self.transform(image),18,0,self.height,self.width)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d63cef60",
      "metadata": {
        "id": "d63cef60"
      },
      "source": [
        "$\\phi$ function stacks 4 images to create a state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d96c97d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6d96c97d",
        "outputId": "bce470fb-b305-4689-d94c-e2c039115a43"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 84, 84])\n"
          ]
        }
      ],
      "source": [
        "test = Preprocess()\n",
        "proc_img = test.process(last_img)\n",
        "print(proc_img.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b03d115",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "id": "2b03d115",
        "outputId": "de038caa-0ba6-4d09-d6d2-5a441f89cdcf",
        "scrolled": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f4ce2e74a00>"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD7CAYAAACscuKmAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAAsTAAALEwEAmpwYAAAQQUlEQVR4nO3dXYwd9X3G8e+zb9gYG3tZart4qV2BQAg54FpgBFQphIbQCHKBEDSqqgrJN2kLJVICrRAN6kWQqiRcRFQGkqKK8lICBFkpKXUcVQjhYMAQY/NiHINtDMumdpe4+3b2/Hoxs2Fx7d3ZnTm75+z/+UirPTPn7Mx/NH48c+bM+f0UEZjZ/Nc21wMws9nhsJslwmE3S4TDbpYIh90sEQ67WSJKhV3S1ZLekrRH0u1VDcrMqqeZfs4uqR14G7gKOAC8BNwUEbuqG56ZVaWjxN9eBOyJiL0Akh4FrgNOGPaenp4488wzJ13o2NgYH3zwAf39/SWGZja/nH766axcuZL29vZJX/f+++/T39+v4z1XJuxnAPsnTB8ALp7sD84880yef/75SRd65MgR7r77bu6//37q9XqJ4ZnND21tbVx//fXceeednHrqqZO+9rLLLjvxcqoe2LEkbZS0XdJ2H63N5k6ZsB8EeidMr8rnfUZEbIqI9RGxvqenp8TqzKyMMmF/CThb0hpJXcCNwDPVDMvMqjbj9+wRUZP0l8BPgXbgBxHxRmUjM7NKlblAR0T8BPhJRWMxswbyHXRmiXDYzRLhsJslwmE3S4TDbpYIh90sEQ67WSIcdrNEOOxmiXDYzRLhsJslwmE3S4TDbpYIh90sEQ67WSIcdrNEOOxmiZgy7JJ+IKlP0s4J87olPSfpnfz3ssYO08zKKnJk/2fg6mPm3Q5siYizgS35tJk1sSnDHhH/Bfz3MbOvAx7KHz8EfKXaYZlZ1Wb6nn15RBzKH38ILK9oPGbWIKUv0EXWGfKE3SHdEcasOcw07B9JWgmQ/+470QvdEcasOcy0bvwzwJ8D385//7iqAbW1tbFs2TJ6e3vd2NEMaG9vZ+nSpbS1lTsRnzLskh4BPg/0SDoA3EUW8scl3Qy8B9xQahQTdHV1cfnll7Ns2TKH3YzsALh27Vq6urpKLWfKsEfETSd46spSaz6Bjo4OVq9ezcKFCxuxeLOWtGLFiil7s0/Fd9CZJcJhN0uEw26WCIfdLBEOu1kiSvVnbwRJdHZ2snDhQrKb88zSJomOjg4klVpOU4Z94cKFdHd3z/VQzJrGokWL5l/YATo7O0vfQGA2n5T9jB38nt0sGQ67WSIcdrNEOOxmiXDYzRLRlFfjgdIfM5jNJ1XkoSnDLslhN5ugijw07Wm8w26WqSoLTRt2M6tWkY4wvZK2Stol6Q1Jt+Tz3RXGrIUUObLXgK9HxHnABuBrks7DXWHMWkqRjjCHIuKV/PEnwG7gDNwVxqylTOtqvKTVwIXANgp2hZG0EdgI0NvbW3Q9vkBnlqsqC4XDLukU4EfArRExMHEAERGSjvvl84jYBGwCWLduXaEvqC9YsIDOzs6iQzOb96r41luhsEvqJAv6wxHxZD77I0krI+LQVF1hpqOtrY2Ojo5KNs5svhgbG6NWq5VaRpEmEQIeBHZHxHcmPNWwrjA+jTerXpEj+6XAnwG/lLQjn/e3NLArjKTSrW7M5pNZuV02Ip4HTrSmhnSFMbPPmte3y5pZtRx2s0Q05bfe6vW6O7iaTVBFHpou7PV6naGhIcbGxuZ6KGZNo729nba2tlIXrpsu7AAjIyOMjIz46G5Gdu9JV1cXCxYsKLWcpgx7vV5ndHR0rodh1hTGxsbo6Cgf1aYM+8jICENDQ27/ZEb2sVt7ezsnn3xyqeU0Zdgjwu/ZzSrmj97MEuGwmyWi6U7jI4J6vU6tVvN7djOy9+xjY2Ol89CUYR8aGmJgYGCuh2LWNE466aTSy2i6sAOMjo4yMjLiI7sZ8/zIPjg4yOHDhx12M7KwL1myZP6FvV6v88knn9DX1+ewm5GFvbu7e/6FHbLAV3HaYjYfSKokC/7ozSwRRTrCLJD0C0mv5R1hvpXPXyNpm6Q9kh6T1NX44ZrZTBU5jR8GroiI3+RVZp+X9O/AbcB3I+JRSf8E3AzcV8Wg/Dm72afGr8aXVaQGXQC/ySc7858ArgD+NJ//EPD3VBD2iKBWqzE8POywm5GFvYqDX9G68e3Ay8BZwPeBd4EjETFeyPoAWUuo4/3ttDvCjI6O+ltvZrnxsJdVKOwRMQZcIGkp8BRwbtEVTLcjTK1W4+2332br1q3+5psZWZWaxYsXs2HDhlJ30k3ro7eIOCJpK3AJsFRSR350XwUcnPEoJqjVauzdu5cXXnjBYTcjC/v5558/Kx1hTgdG86AvBK4C7gG2AtcDj1JxR5jx77O7LJVZZrYKTq4EHsrft7cBj0fEZkm7gEcl/QPwKlmLKDNrUkWuxr9O1qb52Pl7gYsaMSgzq57voDNLhMNulgiH3SwRDrtZIhx2s0Q47GaJcNjNEuGwmyXCYTdLhMNulgiH3SwRDrtZIhx2s0Q47GaJcNjNEuGwmyXCYTdLROGwS2qX9Kqkzfm0O8KYtZDpHNlvAXZPmL6HrCPMWcBhso4wZtakCoVd0irgT4AH8mmRdYR5In/JQ8BXGjA+M6tI0SP794BvAOP1bE9jGh1hJG2XtL2/v7/MWM2shCJdXL8M9EXEyzNZQURsioj1EbG+p6dnJoswswoUqRt/KXCtpGuABcAS4F4a1BHGzBpjyiN7RNwREasiYjVwI/CziPgqn3aEgYo7wphZ9cp8zv5N4DZJe8jew7sjjFkTm25jx58DP88fuyOMWQvxHXRmiXDYzRLhsJslwmE3S4TDbpYIh90sEQ67WSIcdrNEOOxmiXDYzRLhsJslwmE3S4TDbpYIh90sEQ67WSIcdrNEOOxmiShUqUbSPuATYAyoRcR6Sd3AY8BqYB9wQ0Qcbswwzays6RzZ/ygiLoiI9fn07cCWiDgb2JJPm1mTKnMafx1ZJxhwRxizplc07AH8h6SXJW3M5y2PiEP54w+B5cf7Q3eEMWsORavLXhYRByX9DvCcpDcnPhkRISmO94cRsQnYBLBu3brjvsbMGq/QkT0iDua/+4CnyEpIfyRpJUD+u69RgzSz8or0elskafH4Y+CPgZ3AM2SdYMAdYcyaXpHT+OXAU1mXZjqAf42IZyW9BDwu6WbgPeCGxg3TzMqaMux555fPHWf+r4ErGzEoM6ue76AzS4TDbpYIh90sEQ67WSIcdrNEOOxmiXDYzRLhsJslwmE3S4TDbpYIh90sEQ67WSIcdrNEOOxmiXDYzRLhsJslwmE3S0ShsEtaKukJSW9K2i3pEkndkp6T9E7+e1mjB2tmM1f0yH4v8GxEnEtWomo37ghj1lKKVJc9FfhD4EGAiBiJiCO4I4xZSylyZF8DfAz8UNKrkh7IS0q7I4xZCykS9g5gHXBfRFwIHOWYU/aICLIWUf9PRGyKiPURsb6np6fseM1shoqE/QBwICK25dNPkIXfHWHMWsiUYY+ID4H9ks7JZ10J7MIdYcxaStHGjn8FPCypC9gL/AXZfxTuCGPWIgqFPSJ2AOuP85Q7wpi1CN9BZ5YIh90sEQ67WSIcdrNEOOxmiXDYzRLhsJslwmE3S4TDbpYIh90sEQ67WSIcdrNEOOxmiXDYzRLhsJslwmE3S0SRUtLnSNox4WdA0q1uEmHWWorUoHsrIi6IiAuAPwD+F3gKN4kwaynTPY2/Eng3It7DTSLMWsp0w34j8Ej+uFCTCDNrDoXDnleWvRb4t2Ofm6xJhDvCmDWH6RzZvwS8EhEf5dOFmkS4I4xZc5hO2G/i01N4cJMIs5ZStD/7IuAq4MkJs78NXCXpHeAL+bSZNamiTSKOAqcdM+/XuEmEWcvwHXRmiXDYzRLhsJslwmE3S4TDbpYIh90sEQ67WSIcdrNEOOxmiXDYzRLhsJslwmE3S4TDbpYIh90sEQ67WSIcdrNEOOxmiShalupvJL0haaekRyQtkLRG0jZJeyQ9llefNbMmVaT90xnAXwPrI+J8oJ2sfvw9wHcj4izgMHBzIwdqZuUUPY3vABZK6gBOBg4BVwBP5M+7I4xZkyvS6+0g8I/A+2Qh/x/gZeBIRNTylx0AzmjUIM2svCmry+bdWa8D1gBHyDrCXF10BZI2AhsBent7qdVqk76+Xq9zyimnsGLFCsbGxoquZlIRQa1Wo16vMzIywuDgIFkTG7PWMP5vd3h4eNLXTfbvukgp6S8Av4qIjwEkPQlcCiyV1JEf3VcBB0+w8k3AJoC1a9fGwMDApCsbGRnh4osvZsmSJZUFcnh4mL6+Po4ePcq+ffvYsWMHQ0NDlSzbbDYMDAywf/9+Fi9ePOnrRkZGTvhckbC/D2yQdDIwSFYrfjuwFbgeeJSCHWEiYsqQ1et1Vq9eTZWtogYHB9m7dy9HjhxhdHSUnTt3VrZss9kwNDT023+/k5nszLnIe/ZtZBfiXgF+mf/NJuCbwG2S9pA1kHiw8MjnkKS5HoLZnCjaEeYu4K5jZu8FLqp8RGbWEIXCPtuGh4c5evRoZcsbHBxkaGiIoaGhKU+DzOarpgv78PAw27Zt48UXX6zsAl2tVmNgYIDBwUH6+/unvKJpNh81XdhHR0d57bXXePrpp6nX65Uue/w/D3/sZilqurBDFsZ6vV552M1Sptk8ykn6GDgK9M/aShuvB29Ps5pP2wLFtuf3IuL04z0xq2EHkLQ9ItbP6kobyNvTvObTtkD57fH32c0S4bCbJWIuwr5pDtbZSN6e5jWftgVKbs+sv2c3s7nh03izRMxq2CVdLemtvG7d7bO57rIk9UraKmlXXo/vlnx+t6TnJL2T/14212OdDkntkl6VtDmfbtnagpKWSnpC0puSdku6pJX3T9W1H2ct7JLage8DXwLOA26SdN5srb8CNeDrEXEesAH4Wj7+24EtEXE2sCWfbiW3ALsnTLdybcF7gWcj4lzgc2Tb1ZL7pyG1HyNiVn6AS4CfTpi+A7hjttbfgO35MXAV8BawMp+3Enhrrsc2jW1YRRaAK4DNgMhu2ug43j5r5h/gVOBX5NehJsxvyf1DVuZtP9BNdqfrZuCLZfbPbJ7Gjw9+XMvWrZO0GrgQ2AYsj4hD+VMfAsvnalwz8D3gG8D4fcmn0bq1BdcAHwM/zN+WPCBpES26f6IBtR99gW6aJJ0C/Ai4NSI+U2Mrsv9uW+LjDUlfBvoi4uW5HktFOoB1wH0RcSHZbdmfOWVvsf0zsfbj7wKLmEbtx+OZzbAfBHonTJ+wbl2zktRJFvSHI+LJfPZHklbmz68E+uZqfNN0KXCtpH1kpcWuIHvPuzQvGQ6ttY8OAAciq6wEWXWldbTu/vlt7ceIGAU+U/sxf8209s9shv0l4Oz8amIX2cWGZ2Zx/aUoq2f1ILA7Ir4z4alnyGrwQcFafM0gIu6IiFURsZpsX/wsIr7Kp7UFobW250Ngv6Rz8llXArto0f3DhNqP+b+98e2Z+f6Z5YsO1wBvA+8CfzfXF0GmOfbLyE4BXwd25D/XkL3P3QK8A/wn0D3XY53Btn0e2Jw//n3gF8AesrLhJ831+KaxHReQFUN9HXgaWNbK+wf4FvAmsBP4F+CkMvvHd9CZJcIX6MwS4bCbJcJhN0uEw26WCIfdLBEOu1kiHHazRDjsZon4P2XXbdObytqUAAAAAElFTkSuQmCC",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# permute to get correct shape for imshow, imshow expects (height, width, channels)\n",
        "plt.imshow(proc_img.permute(1, 2, 0),cmap='gray')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6469e2d4",
      "metadata": {
        "id": "6469e2d4"
      },
      "source": [
        "### Architecture\n",
        "Based on https://www.nature.com/articles/nature14236\n",
        "\n",
        "The input to the neural network consists of an 84x84x4 image produced by the preprocessing map w. \n",
        "\n",
        "The first hidden layer convolves 32 filters of 8x8 with stride 4 with the\n",
        "input image and applies a rectifier nonlinearity. \n",
        "\n",
        "The second hidden layer convolves 64 filters of 4x4 with stride 2, again followed by a rectifier nonlinearity.\n",
        "\n",
        "This is followed by a third convolutional layer that convolves 64 filters of 3x3 with\n",
        "stride 1 followed by a rectifier. \n",
        "\n",
        "The final hidden layer is fully-connected and consists of 512 rectifier units. The output layer is a fully-connected linear layer with a single output for each valid action. \n",
        "\n",
        "##### We have 4 valid actions!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef15b7f0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# For transfer learning, import a pretrained model and freeze the weights\n",
        "# https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf43a44c",
      "metadata": {
        "id": "cf43a44c"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "\n",
        "class DeepQ(nn.Module):\n",
        "    def __init__(self, input_shape, num_actions):\n",
        "        super(DeepQ, self).__init__()\n",
        "        self.input_shape = input_shape\n",
        "        self.num_actions = num_actions\n",
        "\n",
        "        # initializer, needed for the optimizer\n",
        "        # set a seeder for reproducibility\n",
        "        self.initializer = keras.initializers.VarianceScaling(scale=2.0, seed=0)\n",
        "        \n",
        "        # convolutional layers\n",
        "        self.model = Sequential()\n",
        "        self.model.add(keras.layers.Conv2D(32, (8, 8), strides=(4, 4), activation='relu', input_shape=input_shape, kernel_initializer=self.initializer))\n",
        "        self.model.add(keras.layers.Conv2D(64, (4, 4), strides=(2, 2), activation='relu', kernel_initializer=self.initializer))\n",
        "        self.model.add(keras.layers.Conv2D(64, (3, 3), strides=(1, 1), activation='relu', kernel_initializer=self.initializer))\n",
        "        self.model.add(keras.layers.Flatten())\n",
        "\n",
        "        # problem: expected shape=(None, 1, 210, 160, 3), found shape=(4, 210, 160, 3)\n",
        "        # solution: add a batch dimension to the input\n",
        "        self.model.add(keras.layers.Reshape((1, 210, 160, 3)))\n",
        "        self.model.add(keras.layers.Dense(512, activation='relu', kernel_initializer=self.initializer))\n",
        "        self.model.add(keras.layers.Dense(num_actions, activation='linear', kernel_initializer=self.initializer))\n",
        "\n",
        "        # set optimizer, clipnorm is used to prevent exploding gradients\n",
        "        # Adam is a good default optimizer, it is a combination of RMSProp and SGD\n",
        "        self.optimizer = keras.optimizers.Adam(learning_rate=0.00025, clipnorm=1.0)\n",
        "\n",
        "        # TODO: can use transfer learning to load weights from a pretrained model\n",
        "        #self.model.load_weights('model.h5') # or whatever the path to the weights is\n",
        "\n",
        "    def get_q_values(self, state):\n",
        "        return self.model(state)\n",
        "    \n",
        "    def sample_action(self, state, epsilon): # epsilon greedy action sampling\n",
        "        if random.random() < epsilon:\n",
        "            return random.randint(0, self.num_actions - 1)\n",
        "        else:\n",
        "            with torch.no_grad(): # no need to track gradients for evaluation\n",
        "                return self.get_q_values(state).argmax().item()\n",
        "    \n",
        "    def forward(self, x): # forward pass\n",
        "        return self.model(x)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1fc646e1",
      "metadata": {
        "id": "1fc646e1"
      },
      "outputs": [],
      "source": [
        "# Shape becomes [None, 1, 84, 84] for proc_img\n",
        "test = DeepQ(last_img,env.action_space.n)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "1935ec24",
      "metadata": {},
      "source": [
        "## Suggested by copilot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f89f910",
      "metadata": {
        "id": "0f89f910"
      },
      "outputs": [],
      "source": [
        "# train using the gym environment\n",
        "\n",
        "# hyperparameters\n",
        "num_episodes = 1000\n",
        "batch_size = 32\n",
        "gamma = 0.99\n",
        "learning_rate = 0.00025\n",
        "target_update = 1000\n",
        "replay_memory_size = 100 # 1000000\n",
        "epsilon_start = 1.0\n",
        "epsilon_final = 0.01\n",
        "epsilon_decay = 30000\n",
        "epsilon_by_frame = lambda frame_idx: epsilon_final + (epsilon_start - epsilon_final) * np.exp(-1. * frame_idx / epsilon_decay)\n",
        "\n",
        "# initialize the model\n",
        "input_shape = last_img.shape\n",
        "num_actions = env.action_space.n\n",
        "model = DeepQ(input_shape, num_actions)\n",
        "target_model = DeepQ(input_shape, num_actions)\n",
        "target_model.load_state_dict(model.state_dict())\n",
        "target_model.eval()\n",
        "\n",
        "\n",
        "# initialize the replay memory\n",
        "replay_memory = []\n",
        "\n",
        "# initialize the frame and episode counters\n",
        "frame_idx = 0\n",
        "rewards = []\n",
        "losses = []\n",
        "all_rewards = []\n",
        "episode_reward = 0\n",
        "\n",
        "# initialize the environment\n",
        "env.reset()\n",
        "last_screen = env.render(mode=\"rgb_array\")\n",
        "current_screen = env.render(mode=\"rgb_array\")\n",
        "state = current_screen - last_screen\n",
        "last_screen = current_screen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8077f6d",
      "metadata": {
        "id": "b8077f6d"
      },
      "outputs": [],
      "source": [
        "# start training\n",
        "from itertools import count\n",
        "from collections import namedtuple\n",
        "# Transition is used to store the experience in the replay memory\n",
        "# replay memory is needed for experience replay, which is used to decorrelate the samples\n",
        "Transition = namedtuple('Transition', ('state', 'action', 'reward', 'next_state'))\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "    for t in count():\n",
        "        # select epsilon-greedy action\n",
        "        epsilon = epsilon_by_frame(frame_idx)\n",
        "        # the model is keras, so we need to convert the state to a tensor\n",
        "        state = torch.FloatTensor(state).to(device)\n",
        "        action = model.sample_action(torch.FloatTensor(state).to(device), epsilon)\n",
        "        \n",
        "        # execute action\n",
        "        _, reward, done, _ = env.step(action)\n",
        "        episode_reward += reward\n",
        "        \n",
        "        # observe new state\n",
        "        last_screen = current_screen\n",
        "        current_screen = env.render(mode=\"rgb_array\")\n",
        "        if not done:\n",
        "            next_state = current_screen - last_screen\n",
        "        else:\n",
        "            next_state = None\n",
        "        \n",
        "        # store the transition in the replay memory\n",
        "        replay_memory.append(Transition(state, action, reward, next_state))\n",
        "        replay_memory = replay_memory[-replay_memory_size:] # keep only the last 1000000 transitions\n",
        "        \n",
        "        # update the state\n",
        "        state = next_state\n",
        "        \n",
        "        # optimize the model\n",
        "        if len(replay_memory) > batch_size:\n",
        "            # sample a batch from the replay memory\n",
        "            transitions = random.sample(replay_memory, batch_size)\n",
        "            batch = Transition(*zip(*transitions))\n",
        "            \n",
        "            # compute a mask of non-final states and concatenate the batch elements\n",
        "            non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)\n",
        "            non_final_next_states = torch.FloatTensor([s for s in batch.next_state if s is not None]).to(device)\n",
        "            state_batch = torch.FloatTensor(batch.state).to(device)\n",
        "            action_batch = torch.LongTensor(batch.action).to(device)\n",
        "            reward_batch = torch.FloatTensor(batch.reward).to(device)\n",
        "            \n",
        "            # compute Q(s_t, a)\n",
        "            state_action_values = model(state_batch).gather(1, action_batch.unsqueeze(1))\n",
        "            \n",
        "            # compute V(s_{t+1}) for all next states\n",
        "            next_state_values = torch.zeros(batch_size, device=device)\n",
        "            next_state_values[non_final_mask] = target_model(non_final_next_states).max(1)[0].detach()\n",
        "            \n",
        "            # compute the expected Q values\n",
        "            expected_state_action_values = (next_state_values * gamma) + reward_batch\n",
        "            \n",
        "            # compute loss\n",
        "            loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
        "            \n",
        "            # optimize the model\n",
        "            model.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            for param in model.parameters():\n",
        "                param.grad.data.clamp_(-1, 1)\n",
        "            model.optimizer.step()\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "        frame_idx += 1\n",
        "\n",
        "    # update the target network\n",
        "    if episode % target_update == 0:\n",
        "        target_model.load_state_dict(model.state_dict())\n",
        "\n",
        "    # log results\n",
        "    rewards.append(episode_reward)\n",
        "    losses.append(loss.item())\n",
        "    mean_100ep_reward = round(np.mean(rewards[-100:]), 1)\n",
        "    mean_100ep_loss = round(np.mean(losses[-100:]), 5)\n",
        "    print('Episode {}\\tLast reward: {:.2f}\\tAverage reward (last 100 episodes): {:.2f}\\tLoss: {:.5f}'.format(episode, episode_reward, mean_100ep_reward, mean_100ep_loss))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b533bb66",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Delete the model from memory to avoid errors\n",
        "del model"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
